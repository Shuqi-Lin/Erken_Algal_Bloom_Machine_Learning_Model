{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "153cdc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import random\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,confusion_matrix\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler,PowerTransformer,StandardScaler\n",
    "from math import sqrt\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', 1000)  # or 1000\n",
    "pd.set_option('display.max_rows', 1000)  # or 1000\n",
    "import time\n",
    "from numpy import concatenate\n",
    "import matplotlib.pyplot as plt\n",
    "Scaler_X = MinMaxScaler()\n",
    "Scaler_y = PowerTransformer(standardize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5934850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73861073",
   "metadata": {},
   "source": [
    "# Specify the path and folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6198b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you are in the main folder('..\\Erken_Algal_Bloom_Machine_Learning_Model')\n",
    "cd = os.getcwd()\n",
    "while cd.split('\\\\')[-1]!='Erken_Algal_Bloom_Machine_Learning_Model':\n",
    "    os.chdir('..')\n",
    "    cd=os.getcwd()\n",
    "Observation =cd+'\\\\Training data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b283bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#infile_path = cd+'\\PDF_Uppsala\\Training dataset\\Process-based model_Jorrit'\n",
    "LSTM_save_folder = cd+'\\\\Work record\\\\3-Sample sparsity test\\\\LSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd53e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, hyperparameters, var_name,dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1] # number of variables\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(hyperparameters['time_steps'], 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [(var_name[j]+'(t-%d)' % (i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, hyperparameters['n_out']):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [(var_name[j]+'(t)') for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [(var_name[j]+'(t+%d)' % (i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    return agg\n",
    "\n",
    "# load dataset\n",
    "def load_dataset(df,var_name):\n",
    "    values = df[var_name].values\n",
    "    return values\n",
    "\n",
    "# reframe dataset\n",
    "def reframe(values,hyperparameters,var_names):\n",
    "    reframed = series_to_supervised(values, hyperparameters,var_names)\n",
    "    reframed = reframed.iloc[hyperparameters['time_steps']:]\n",
    "    drop_col =[]\n",
    "    for i in range(1,hyperparameters['time_steps']+1):\n",
    "        drop_col += [len(var_name)*i-1]\n",
    "    reframed.drop(reframed.iloc[:,drop_col],axis=1,inplace = True)\n",
    "    return reframed\n",
    "\n",
    "def sparse_dataset(data_X,data_y):\n",
    "    index = []\n",
    "    y = []\n",
    "    for i in range(len(data_y)):\n",
    "        if ~np.isnan(data_y[i]):\n",
    "            index.append(i)\n",
    "            y.append(data_y[i])\n",
    "    X = np.stack(data_X[index,:,:])\n",
    "    y = np.array(y)\n",
    "    return index,X,y\n",
    "\n",
    "def fit_lstm(train_X,train_y,n_batch,nb_epoch,n_neuros,dropout,verbose,loss_function):\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_neuros,  return_sequences = True,\n",
    "              input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(n_neuros, return_sequences = True))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(n_neuros))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=loss_function, optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit(train_X,train_y,epochs =nb_epoch,batch_size = n_batch,verbose = verbose)\n",
    "    return model\n",
    "\n",
    "def split_dataset(train,test,time_steps):\n",
    "    # split into input and outputs\n",
    "    train_X, train_y = train[:, :-1], train[:, -1]\n",
    "    test_X, test_y = test[:, :-1], test[:, -1]\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], time_steps+1, int(train_X.shape[1]/(time_steps+1))))\n",
    "    test_X = test_X.reshape((test_X.shape[0], time_steps+1, int(test_X.shape[1]/(time_steps+1))))\n",
    "    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "    print('number of input timesteps: {}'.format(train_X.shape[1]))\n",
    "    print('number of features: {}'.format(train_X.shape[2]))\n",
    "    return train_X, train_y,test_X, test_y\n",
    "\n",
    "# ensure all data is float\n",
    "def predict_lstm(Nut_memory,test_yr,var_name,nutrient,hyperparameters):\n",
    "    # frame as supervised learning\n",
    "    # scale the training dataset\n",
    "    Nut_memory['year']=Nut_memory['Date'].apply(lambda d:d.year)\n",
    "    values = load_dataset(Nut_memory[~Nut_memory['year'].isin(test_yr)],var_name) #values = values.astype('float32')\n",
    "    n_var = len(var_name)\n",
    "    reframed = reframe(values,hyperparameters,var_name)\n",
    "    reframed_scaled=pd.DataFrame(Scaler_X.fit_transform(reframed.iloc[:,:-1]),\n",
    "                                 columns=reframed.columns[:-1])\n",
    "    target_scaled=pd.Series(Scaler_y.fit_transform(reframed.iloc[:,-1].values.reshape(-1, 1)).reshape(-1),\n",
    "                 name=reframed.columns[-1])\n",
    "    reframed_scaled=pd.concat([reframed_scaled,target_scaled],axis=1)\n",
    "    train = reframed_scaled.values\n",
    "    \n",
    "    # scale the testing dataset\n",
    "    values = load_dataset(Nut_memory[Nut_memory['year'].isin(test_yr)],var_name) #values = values.astype('float32')\n",
    "    reframed = reframe(values,hyperparameters,var_name)\n",
    "    reframed_scaled=pd.DataFrame(Scaler_X.fit_transform(reframed.iloc[:,:-1]),\n",
    "                                 columns=reframed.columns[:-1])\n",
    "    target_scaled=pd.Series(Scaler_y.fit_transform(reframed.iloc[:,-1].values.reshape(-1, 1)).reshape(-1),\n",
    "                 name=reframed.columns[-1])\n",
    "    reframed_scaled=pd.concat([reframed_scaled,target_scaled],axis=1)\n",
    "    test = reframed_scaled.values\n",
    "    train_X, train_y,test_X, test_y = split_dataset(train,test,hyperparameters['time_steps'])\n",
    "    # fit the lstm model\n",
    "    index,X,y = sparse_dataset(train_X,train_y) # stack the timeseries input together to create a 2D training input X, and a 1D lable y\n",
    "    print('number of samples: {}'.format(len(index)))\n",
    "    model = fit_lstm(X,y,\n",
    "                     hyperparameters['n_batch'],hyperparameters['nb_epoch'],\n",
    "                     hyperparameters['n_neuros'],hyperparameters['dropout'],\n",
    "                     hyperparameters['verbose'],hyperparameters['loss_function'])\n",
    "    yhat = Scaler_y.inverse_transform(model.predict(test_X,batch_size = hyperparameters['n_batch']))\n",
    "    x = np.empty((hyperparameters['time_steps'],1))\n",
    "    x[:] = np.nan\n",
    "    yhat = np.concatenate((x,yhat))\n",
    "    df = pd.DataFrame({'Date':Nut_memory[Nut_memory['year'].isin(test_yr)]['Date'],nutrient:yhat.flatten()})\n",
    "    return df,model,yhat\n",
    "\n",
    "def compare(Erken_Nut,Nut_memory,nutrient,MAE,RMSE,R2,hat):\n",
    "    compare = Erken_Nut.merge(Nut_memory,on = 'Date',how = 'left')[['Date',nutrient+'_x',nutrient+'_y']].dropna()\n",
    "    compare.columns = [['Date','ML','OB']]\n",
    "    mae = mean_absolute_error(compare['OB'], compare['ML'])\n",
    "    rmse = mean_squared_error(compare['OB'], compare['ML'],squared=False)\n",
    "    r2 = r2_score(compare['OB'], compare['ML'])\n",
    "    MAE.append(mae)\n",
    "    RMSE.append(rmse)\n",
    "    R2.append(r2)\n",
    "    # Add the time-series prediction into sample dataset for next variable modeling\n",
    "    Nut_memory.loc[Nut_memory['year'].isin(test_yr),nutrient] = hat\n",
    "    return MAE,RMSE,R2,Nut_memory\n",
    "\n",
    "def predict_ts(df,nutrient,model,var_name,hyperparameters):\n",
    "    values = load_dataset(df,var_name) #values = values.astype('float32')\n",
    "    # frame as supervised learning\n",
    "    n_var = len(var_name)\n",
    "    reframed = reframe(values,hyperparameters,var_name)\n",
    "    values = reframed.values\n",
    "    \n",
    "    # add the predictive values into dataset\n",
    "    value_X, value_y = values[:, :-1], values[:, -1]\n",
    "    value_X = value_X.reshape((value_X.shape[0], hyperparameters['time_steps']+1, int(value_X.shape[1]/(hyperparameters['time_steps']+1))))\n",
    "    y_pred = Scaler_y.inverse_transform(model.predict(value_X,batch_size = hyperparameters['n_batch']))    \n",
    "    df[nutrient].iloc[hyperparameters['time_steps']:]=y_pred[:,0]\n",
    "    df[nutrient].fillna(method = 'backfill',inplace = True)\n",
    "    return df\n",
    "\n",
    "# Load testing dataset(Daily data)\n",
    "def read_daily_df(features,file):\n",
    "    Lake_HydMet = pd.read_csv(file,header = 0,sep = '\\t',parse_dates = ['Date'])\n",
    "    Lake_HydMet = Lake_HydMet[features]\n",
    "    return Lake_HydMet\n",
    "\n",
    "def resample(Nut,Erken_Met,sample_interval,test_yr):\n",
    "    # Resample the dataset\n",
    "    drop_row = []\n",
    "    dT = 0\n",
    "    i = 1\n",
    "    Nut_sample = Nut.copy()\n",
    "    Nut_sample['year'] = Nut_sample['Date'].apply(lambda yr:yr.year)\n",
    "    Nut_train = Nut_sample[~Nut_sample['year'].isin(test_yr)]\n",
    "    Nut_test = Nut_sample[Nut_sample['year'].isin(test_yr)]\n",
    "    while True:\n",
    "        dT = (Nut_train['Date'].iloc[i]-Nut_train['Date'].iloc[i-1])/np.timedelta64(1, 'D')\n",
    "        if dT<sample_interval: # set the data frequency (over 10, 14, 20 days)\n",
    "            #print(sample_df['Date'].iloc[i])\n",
    "            Nut_train.drop(Nut_train.index.values[i],axis = 0,inplace  =True)\n",
    "        else:\n",
    "            i+=1\n",
    "        if i>=len(Nut_train):break\n",
    "    Nut_sample = pd.concat([Nut_train,Nut_test]).sort_values(by= 'Date')\n",
    "    #Nut_memory = Erken_Met.merge(Nut_sample,how = 'left',on = 'Date')\n",
    "    #Nut_memory['year'] = Nut_memory['Date'].apply(lambda d:d.year)\n",
    "    return Nut_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30e91a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "os.chdir(Observation)\n",
    "lakename='Erken'\n",
    "all_df = pd.read_csv(lakename+'_Observation_df_nowinter.csv',sep = '\\t',parse_dates=['Date'])\n",
    "# Define the features and years\n",
    "year = [2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020]\n",
    "features = ['Date','U','SST','AirT','delT','Humidity','CC','SWR',\n",
    "            'Prec','inflow','outflow','Ice_d','days from iceoff','MLD','W','thermD']\n",
    "# Load daily physical factors\n",
    "file = lakename+'_Daily_Observation_df_nowinter.csv'\n",
    "Erken_Met = read_daily_df(features,file) # choose ice data or not\n",
    "Erken_Met=Erken_Met[Erken_Met['Date']>pd.Timestamp(year[0],1,1)]\n",
    "\n",
    "# Create the daily df with Nan in nutrients columns\n",
    "Nut = all_df[['Date','Si','TotP','NH4','NOX','PO4','O2','Chl']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23d7b666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold of delta_Chl:0.35\n",
      "Threshold of Chl:5\n"
     ]
    }
   ],
   "source": [
    "# Use the delta_Chl threshold to find the onset dates of algal bloom\n",
    "threshold1 = float(input('Threshold of delta_Chl:')) #0.35\n",
    "threshold2 = float(input('Threshold of Chl:')) #5\n",
    "def find_bloom(x):\n",
    "    if (x['delta_Chl']>threshold1) &(x['Chl']>threshold2):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "sdate = pd.Timestamp(2004,1,1)\n",
    "edate = pd.Timestamp(2020,12,31)\n",
    "\n",
    "OB=all_df[['Date','Chl']]\n",
    "OB=OB[~OB['Date'].duplicated()]\n",
    "OB_int = OB.set_index('Date').reindex(pd.date_range(start = sdate,end = edate,freq = '1D')).interpolate(method = 'linear')\n",
    "OB_int = OB_int.reset_index()\n",
    "OB_int.columns = ['Date','Chl']\n",
    "OB_int['Date'] = OB_int['Date'].apply(lambda d:d.date())\n",
    "OB_int['Date'] = pd.to_datetime(OB_int['Date'])\n",
    "# Use the delta_Chl threshold to find the onset dates of algal bloom\n",
    "OB_int['delta_Chl'] = np.diff(OB_int['Chl'],append=0)\n",
    "OB_int['Bloom'] = OB_int.apply(lambda x: find_bloom(x), axis=1) \n",
    "OB_bloom = OB_int[['Date','Bloom']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eddcedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Erken_NOX = pd.DataFrame(columns = ['Date','NOX'])\n",
    "Erken_O2 = pd.DataFrame(columns = ['Date','O2'])\n",
    "Erken_NH4 = pd.DataFrame(columns = ['Date','NH4'])\n",
    "Erken_PO4 = pd.DataFrame(columns = ['Date','PO4'])\n",
    "Erken_TP = pd.DataFrame(columns = ['Date','TP'])\n",
    "Erken_Si = pd.DataFrame(columns = ['Date','Si'])\n",
    "Erken_Chl = pd.DataFrame(columns = ['Date','Chl'])\n",
    "\n",
    "NOX_MAE = []\n",
    "NOX_RMSE = []\n",
    "NOX_R2 = []\n",
    "O2_MAE = []\n",
    "O2_RMSE = []\n",
    "O2_R2 = []\n",
    "NH4_MAE = []\n",
    "NH4_RMSE = []\n",
    "NH4_R2 = []\n",
    "PO4_MAE = []\n",
    "PO4_RMSE = []\n",
    "PO4_R2 = []\n",
    "TP_MAE = []\n",
    "TP_RMSE = []\n",
    "TP_R2 = []\n",
    "Si_MAE = []\n",
    "Si_RMSE = []\n",
    "Si_R2 = []\n",
    "Chl_MAE = []\n",
    "Chl_RMSE = []\n",
    "Chl_R2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "167418ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyperparameters\n",
    "hyperparameters = {'n_batch':10,'nb_epoch':100,'n_neuros':100,'dropout':0.01,'time_steps':7,\n",
    "                   'n_out':1,'verbose':0,'loss_function':'mae'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "id": "504acd07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shuli278\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOX\n",
      "(3930, 8, 14) (3930,) (1206, 8, 14) (1206,)\n",
      "number of input timesteps: 8\n",
      "number of features: 14\n",
      "number of samples: 87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shuli278\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "O2\n",
      "(3929, 8, 15) (3929,) (1206, 8, 15) (1206,)\n",
      "number of input timesteps: 8\n",
      "number of features: 15\n",
      "number of samples: 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shuli278\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "PO4\n",
      "(3929, 8, 16) (3929,) (1206, 8, 16) (1206,)\n",
      "number of input timesteps: 8\n",
      "number of features: 16\n",
      "number of samples: 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shuli278\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TotP\n",
      "(3907, 8, 17) (3907,) (1206, 8, 17) (1206,)\n",
      "number of input timesteps: 8\n",
      "number of features: 17\n",
      "number of samples: 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shuli278\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "NH4\n",
      "(3907, 8, 18) (3907,) (1206, 8, 18) (1206,)\n",
      "number of input timesteps: 8\n",
      "number of features: 18\n",
      "number of samples: 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shuli278\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Si\n",
      "(3870, 8, 19) (3870,) (1206, 8, 19) (1206,)\n",
      "number of input timesteps: 8\n",
      "number of features: 19\n",
      "number of samples: 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shuli278\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Chl\n",
      "(3870, 8, 20) (3870,) (1206, 8, 20) (1206,)\n",
      "number of input timesteps: 8\n",
      "number of features: 20\n",
      "number of samples: 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shuli278\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "c:\\users\\shuli278\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016, 2017, 2018, 2019]\n",
      "Model takes 18 min to run\n",
      "\n",
      "\n",
      "NOX\n",
      "(3930, 8, 14) (3930,) (1206, 8, 14) (1206,)\n",
      "number of input timesteps: 8\n",
      "number of features: 14\n",
      "number of samples: 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shuli278\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "O2\n",
      "(3930, 8, 15) (3930,) (1169, 8, 15) (1169,)\n",
      "number of input timesteps: 8\n",
      "number of features: 15\n",
      "number of samples: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shuli278\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "PO4\n",
      "(3930, 8, 16) (3930,) (1169, 8, 16) (1169,)\n",
      "number of input timesteps: 8\n",
      "number of features: 16\n",
      "number of samples: 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shuli278\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TotP\n",
      "(3930, 8, 17) (3930,) (1169, 8, 17) (1169,)\n",
      "number of input timesteps: 8\n",
      "number of features: 17\n",
      "number of samples: 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shuli278\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "NH4\n",
      "(3930, 8, 18) (3930,) (1144, 8, 18) (1144,)\n",
      "number of input timesteps: 8\n",
      "number of features: 18\n",
      "number of samples: 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shuli278\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Si\n",
      "(3930, 8, 19) (3930,) (1144, 8, 19) (1144,)\n",
      "number of input timesteps: 8\n",
      "number of features: 19\n",
      "number of samples: 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shuli278\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Chl\n",
      "(3930, 8, 20) (3930,) (1144, 8, 20) (1144,)\n",
      "number of input timesteps: 8\n",
      "number of features: 20\n",
      "number of samples: 87\n",
      "[2017, 2018, 2019, 2020]\n",
      "Model takes 14 min to run\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shuli278\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(year)-3):#len(year)-3\n",
    "    start_time = time.time()\n",
    "    os.chdir(Observation)\n",
    "    test_yr = year[i:i+4]\n",
    "    training_yr = [yr for yr in year if yr not in test_yr]\n",
    "    Nut = resample(Nut,Erken_Met,35,test_yr)\n",
    "    \n",
    "    # set the directory\n",
    "    os.chdir(LSTM_save_folder)\n",
    "    \n",
    "    # predict NOX\n",
    "    nutrient = 'NOX'\n",
    "    print(nutrient)\n",
    "    var_name = ['delT', 'Ice_d','days from iceoff', 'inflow','outflow','U', 'AirT', \n",
    "                'Humidity', 'CC','SWR','Prec','MLD','W','thermD',nutrient] #'MLD','W','thermD',\n",
    "    Scaler_X = MinMaxScaler()\n",
    "    Scaler_y = PowerTransformer(standardize=False)\n",
    "    \n",
    "    Nut_memory = Erken_Met.merge(pd.concat([Nut['Date'],Nut[nutrient]],axis = 1).dropna().drop_duplicates('Date'),\n",
    "                            how = 'left',on = 'Date')\n",
    "\n",
    "    NOX_df,NOX_model,NOX_hat = predict_lstm(Nut_memory,test_yr,var_name,nutrient,hyperparameters)\n",
    "    # Compare the predictions with observations\n",
    "    NOX_MAE,NOX_RMSE,NOX_R2,Nut_memory = compare(NOX_df,Nut_memory,nutrient,NOX_MAE,NOX_RMSE,NOX_R2,NOX_hat)\n",
    "    Erken_NOX = NOX_df \n",
    "    \n",
    "    ## use the trained model to interplate the whole timeseries\n",
    "    Nut_memory = predict_ts(Nut_memory,nutrient,NOX_model,var_name,hyperparameters)\n",
    "    \n",
    "    # predict O2\n",
    "    nutrient = 'O2'\n",
    "    print('\\n')\n",
    "    print(nutrient)\n",
    "    var_name = ['delT', 'Ice_d','days from iceoff','inflow','outflow','U', 'AirT', \n",
    "                'Humidity','CC','SWR','Prec','MLD','W','thermD','NOX',nutrient] # removed 'month' and 'SST'\n",
    "    Scaler_X = MinMaxScaler()\n",
    "    Scaler_y = MinMaxScaler()\n",
    "    Nut_memory = Nut_memory.dropna().merge(pd.concat([Nut['Date'],Nut[nutrient]],axis = 1).dropna().drop_duplicates('Date')\n",
    "                              ,how = 'left',on = 'Date')\n",
    "    \n",
    "    O2_df,O2_model,O2_hat = predict_lstm(Nut_memory,test_yr,var_name,nutrient,hyperparameters)\n",
    "    # Compare the predictions with observations\n",
    "    O2_MAE,O2_RMSE,O2_R2,Nut_memory = compare(O2_df,Nut_memory,nutrient,O2_MAE,O2_RMSE,O2_R2,O2_hat)\n",
    "    Erken_O2 = O2_df\n",
    "    ## use the trained model to interplate the whole timeseries\n",
    "    Nut_memory = predict_ts(Nut_memory,nutrient,O2_model,var_name,hyperparameters)\n",
    "    \n",
    "    # predict PO4\n",
    "    nutrient = 'PO4'\n",
    "    print('\\n')\n",
    "    print(nutrient)\n",
    "    var_name = ['delT', 'Ice_d','days from iceoff', 'inflow','outflow','U', 'AirT', \n",
    "                'Humidity', 'CC','SWR','Prec','MLD','W','thermD','NOX','O2', nutrient] \n",
    "    Scaler_X = MinMaxScaler()\n",
    "    Scaler_y = PowerTransformer(standardize=False)\n",
    "    Nut_memory = Nut_memory.dropna().merge(pd.concat([Nut['Date'],Nut[nutrient]],axis = 1).dropna().drop_duplicates('Date')\n",
    "                              ,how = 'left',on = 'Date')\n",
    "    PO4_df,PO4_model,PO4_hat = predict_lstm(Nut_memory,test_yr,var_name,nutrient,hyperparameters)\n",
    "    # Compare the predictions with observations\n",
    "    PO4_MAE,PO4_RMSE,PO4_R2,Nut_memory = compare(PO4_df,Nut_memory,nutrient,PO4_MAE,PO4_RMSE,PO4_R2,PO4_hat)\n",
    "    Erken_PO4 = PO4_df #pd.concat([Erken_PO4,PO4_df])\n",
    "     ## use the trained model to interplate the whole timeseries\n",
    "    Nut_memory = predict_ts(Nut_memory,nutrient,PO4_model,var_name,hyperparameters)\n",
    "   \n",
    "    # Predict TP\n",
    "    nutrient = 'TotP'\n",
    "    print('\\n')\n",
    "    print(nutrient)\n",
    "    var_name = ['delT', 'Ice_d','days from iceoff','inflow','outflow','U','AirT',\n",
    "                'Humidity','CC','SWR','Prec','MLD','W','thermD','NOX','PO4','O2', nutrient] \n",
    "    Scaler_X = MinMaxScaler()\n",
    "    Scaler_y = PowerTransformer(standardize=False)\n",
    "    Nut_memory = Nut_memory.dropna().merge(pd.concat([Nut['Date'],Nut[nutrient]],axis = 1).dropna().drop_duplicates('Date')\n",
    "                              ,how = 'left',on = 'Date')\n",
    "    TP_df,TP_model,TP_hat = predict_lstm(Nut_memory,test_yr,var_name,nutrient,hyperparameters)\n",
    "    # Compare the predictions with observations\n",
    "    TP_MAE,TP_RMSE,TP_R2,Nut_memory = compare(TP_df,Nut_memory,nutrient,TP_MAE,TP_RMSE,TP_R2,TP_hat)\n",
    "    Erken_TP = TP_df#pd.concat([Erken_TP,TP_df])\n",
    "    ## use the trained model to interplate the whole timeseries\n",
    "    Nut_memory = predict_ts(Nut_memory,nutrient,TP_model,var_name,hyperparameters)\n",
    "    \n",
    "    # predict NH4\n",
    "    nutrient = 'NH4'\n",
    "    print('\\n')\n",
    "    print(nutrient)\n",
    "    var_name = ['delT', 'Ice_d','days from iceoff', 'inflow','outflow','U', 'AirT', \n",
    "                'Humidity', 'CC','SWR','Prec','MLD','W','thermD','NOX','O2','PO4','TotP',nutrient] \n",
    "    Scaler_X = MinMaxScaler()\n",
    "    Scaler_y = PowerTransformer(standardize=False)\n",
    "    Nut_memory = Nut_memory.dropna().merge(pd.concat([Nut['Date'],Nut[nutrient]],axis = 1).dropna().drop_duplicates('Date')\n",
    "                              ,how = 'left',on = 'Date')\n",
    "    NH4_df,NH4_model,NH4_hat = predict_lstm(Nut_memory,test_yr,var_name,nutrient,hyperparameters)\n",
    "    # Compare the predictions with observations\n",
    "    NH4_MAE,NH4_RMSE,NH4_R2,Nut_memory = compare(NH4_df,Nut_memory,nutrient,NH4_MAE,NH4_RMSE,NH4_R2,NH4_hat)\n",
    "    Erken_NH4 = NH4_df#pd.concat([Erken_NH4,NH4_df])\n",
    "    ## use the trained model to interplate the whole timeseries\n",
    "    Nut_memory = predict_ts(Nut_memory,nutrient,NH4_model,var_name,hyperparameters)\n",
    "    \n",
    "    # Predict Si\n",
    "    nutrient = 'Si'\n",
    "    print('\\n')\n",
    "    print(nutrient)\n",
    "    var_name = ['delT', 'Ice_d','days from iceoff','inflow','outflow','U','AirT',\n",
    "                'Humidity','CC','SWR','Prec','MLD','W','thermD',\n",
    "                'NOX','PO4','O2','TotP','NH4', nutrient] \n",
    "    Scaler_X = MinMaxScaler()\n",
    "    Scaler_y = MinMaxScaler()#PowerTransformer(standardize=False)\n",
    "    Nut_memory = Nut_memory.dropna().merge(pd.concat([Nut['Date'],Nut[nutrient]],axis = 1).dropna().drop_duplicates('Date')\n",
    "                              ,how = 'left',on = 'Date')\n",
    "    Si_df,Si_model,Si_hat = predict_lstm(Nut_memory,test_yr,var_name,nutrient,hyperparameters)\n",
    "    # Compare the predictions with observations\n",
    "    Si_MAE,Si_RMSE,Si_R2,Nut_memory = compare(Si_df,Nut_memory,nutrient,Si_MAE,Si_RMSE,Si_R2,Si_hat)\n",
    "    Erken_Si = Si_df #pd.concat([Erken_Si,Si_df])\n",
    "    ## use the trained model to interplate the whole timeseries\n",
    "    Nut_memory = predict_ts(Nut_memory,nutrient,Si_model,var_name,hyperparameters)\n",
    "\n",
    "    # Predict Chl\n",
    "    nutrient = 'Chl'\n",
    "    print('\\n')\n",
    "    print(nutrient)\n",
    "    var_name = ['delT', 'Ice_d','days from iceoff','inflow','outflow','U','AirT',\n",
    "                'Humidity','CC','SWR','Prec','MLD','W','thermD',\n",
    "                'NOX','PO4','TotP','O2','Si','NH4',nutrient] #'Si','NH4',\n",
    "    Scaler_X = MinMaxScaler()\n",
    "    Scaler_y = MinMaxScaler()#PowerTransformer(standardize=False)\n",
    "    Nut_memory = Nut_memory.dropna().merge(pd.concat([Nut['Date'],Nut[nutrient]],axis = 1).dropna().drop_duplicates('Date')\n",
    "                              ,how = 'left',on = 'Date')\n",
    "    Chl_df,Chl_model,Chl_hat = predict_lstm(Nut_memory,test_yr,var_name,nutrient,hyperparameters)\n",
    "    # Compare the predictions with observations\n",
    "    Chl_MAE,Chl_RMSE,Chl_R2,Nut_memory = compare(Chl_df,Nut_memory,nutrient,Chl_MAE,Chl_RMSE,Chl_R2,Chl_hat)\n",
    "    Erken_Chl = Chl_df #pd.concat([Erken_Chl,Chl_df])\n",
    "    ## use the trained model to interplate the whole timeseries\n",
    "    Nut_memory = predict_ts(Nut_memory,nutrient,Chl_model,var_name,hyperparameters)\n",
    "    Erken_Nut = Erken_NOX.merge(Erken_NH4,\n",
    "                on='Date',\n",
    "                how = 'left').merge(Erken_O2,\n",
    "                                    on='Date',\n",
    "                                    how = 'left').merge(Erken_PO4,\n",
    "                                                        on='Date',\n",
    "                                                        how = 'left').merge(Erken_TP,\n",
    "                                                                            on = 'Date',\n",
    "                                                                            how='left').merge(Erken_Si,\n",
    "                                                                                              on='Date',\n",
    "                                                                                              how = 'left').merge(Erken_Chl,\n",
    "                                                                                                                  on='Date',\n",
    "                                                                                                                  how = 'left')\n",
    "    \n",
    "    \n",
    "    os.chdir(LSTM_save_folder)\n",
    "    Erken_Nut.to_csv('LSTM predicted nutrient_predicted nutrient and Chl_'+str(year[i])+'.csv',index = False)\n",
    "        \n",
    "    print(test_yr)\n",
    "    print('Model takes '+str(round((time.time()-start_time)/60))+' min to run')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c936678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(LSTM_save_folder)\n",
    "sample_interval=int(input('Sample interval: ')) # 7\n",
    "rolling_interval=int(input('Rolling interval: ')) # 3\n",
    "rolling_threshold=float(input('Rolling bloom threshold: ')) #0.01\n",
    "\n",
    "TPR=[]\n",
    "FPR=[]\n",
    "Kappa=[]\n",
    "\n",
    "for i in range(len(year)-3):#\n",
    "    ML_Nut=pd.read_csv('LSTM predicted nutrient_predicted nutrient and Chl_'+str(year[i])+'.csv',\n",
    "                       parse_dates=['Date'])\n",
    "    ML_Nut['NOX']=ML_Nut['NOX'].rolling(sample_interval,min_periods = 1).mean()\n",
    "    ML_Nut['NH4']=ML_Nut['NH4'].rolling(sample_interval,min_periods = 1).mean()\n",
    "    ML_Nut['O2']=ML_Nut['O2'].rolling(sample_interval,min_periods = 1).mean()\n",
    "    ML_Nut['PO4']=ML_Nut['PO4'].rolling(sample_interval,min_periods = 1).mean()\n",
    "    ML_Nut['TotP']=ML_Nut['TotP'].rolling(sample_interval,min_periods = 1).mean()\n",
    "    ML_Nut['Si']=ML_Nut['Si'].rolling(sample_interval,min_periods = 1).mean()\n",
    "    ML_Nut['Chl']=ML_Nut['Chl'].rolling(sample_interval,min_periods = 1).mean()\n",
    "    ML_Nut.to_csv('LSTM predicted nutrient_predicted nutrient and Chl_'+str(year[i])+'.csv',index=False)\n",
    "    ML_Chl = ML_Nut[['Date','Chl']].sort_values(by='Date')\n",
    "    ML_Chl['Date'] = pd.to_datetime(ML_Chl['Date'])\n",
    "    ts=pd.DataFrame(pd.date_range(start=pd.Timestamp(year[i],1,1),\n",
    "                                  end=pd.Timestamp(year[i]+3,12,31),\n",
    "                                  freq='D'),\n",
    "                    columns=['Date'])\n",
    "    ML_Chl=ts.merge(ML_Chl,on='Date',how='left')\n",
    "    ML_Chl['delta_Chl'] = np.diff(ML_Chl['Chl'],append = 0)\n",
    "    ML_Chl['Bloom'] = ML_Chl.apply(lambda x: find_bloom(x),axis=1)\n",
    "    ML_bloom = ML_Chl[['Date','Bloom']]\n",
    "\n",
    "    ML_bloom['rolling_bloom']=ML_bloom['Bloom'].rolling(rolling_interval,\n",
    "                                                      center=False,\n",
    "                                                      closed='both').mean().apply(lambda x: 1 if x>=rolling_threshold else 0)\n",
    "    # Find the model hits the events or not\n",
    "    Event_detection = OB_bloom.merge(ML_bloom[['Date','rolling_bloom']],how = 'inner',on = 'Date')\n",
    "    Event_detection.columns = ['Date','OB','ML']\n",
    "    # Compute the metrics\n",
    "    tn, fp, fn, tp=confusion_matrix(Event_detection['OB'],Event_detection['ML']).ravel() \n",
    "    (tn, fp, fn, tp)\n",
    "    print('TPR: {}'.format(round(tp/(tp+fn),2)))\n",
    "    print('FPR: {}'.format(round(fp/(fp+tn),2)))\n",
    "    accuracy=(tn+tp)/(tn+tp+fn+fp)\n",
    "    pe=((tp+fp)/(tn+tp+fn+fp)*(fn+tn)/(tn+tp+fn+fp))+((tp+fn)/(tn+tp+fn+fp)*(fp+tn)/(tn+tp+fn+fp))\n",
    "    print('Kappa: {}'.format(round((accuracy-pe)/(1-pe),2)))\n",
    "    TPR.append(tp/(tp+fn))\n",
    "    FPR.append(fp/(fp+tn))\n",
    "    Kappa.append((accuracy-pe)/(1-pe))\n",
    "\n",
    "    f,ax=plt.subplots(figsize=(12,6))\n",
    "    ax.plot(ML_Chl['Date'],ML_Chl['Chl'],label = 'Observation')\n",
    "    ax.plot(OB['Date'],OB['Chl'],'g*',label = 'GBR')\n",
    "\n",
    "    ax.plot(OB_bloom[OB_bloom['Bloom']==1]['Date'],\n",
    "           OB_bloom[OB_bloom['Bloom']==1]['Bloom']+0.5,'rs',ms=5,alpha=0.5,label = 'Observed bloom')\n",
    "    ax.plot(ML_Chl[ML_Chl['Bloom']==1]['Date'],\n",
    "            ML_Chl[ML_Chl['Bloom']==1]['Bloom']-0.5,'ks',ms=5,alpha=0.5,label = 'LSTM bloom')\n",
    "\n",
    "    ax.set_xlim(pd.Timestamp(year[i],1,1),pd.Timestamp(year[i]+3,12,31))\n",
    "    ax.set_ylim(0,45)\n",
    "    ax.tick_params(axis='x', labelsize= 16,rotation=30)\n",
    "    ax.tick_params(axis='y', labelsize= 16)\n",
    "    ax.set_ylabel('Chl $mg/m^{3}$',fontsize=20)\n",
    "    ax.grid(axis = 'x')\n",
    "    ax.legend(ncol=2,loc=9,frameon=False,fontsize=16)\n",
    "    f.savefig(str(year[i])+'.png',dpi=300)\n",
    "\n",
    "#ML_Nut.to_csv('Erken_GBR predicted nutrient and Chl.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "98554d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=pd.DataFrame({'TPR':TPR,'FPR':FPR,'Kappa':Kappa},index=year[:14])\n",
    "metrics.to_csv('metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "470aa5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(LSTM_save_folder)\n",
    "Chl_MAE = []\n",
    "Chl_RMSE = []\n",
    "Chl_R2 = []\n",
    "for i in range(len(year)-3):\n",
    "    ML_Nut=pd.read_csv('LSTM predicted nutrient_predicted nutrient and Chl_'+str(year[i])+'.csv',\n",
    "                       parse_dates=['Date'])\n",
    "    ML_Chl = ML_Nut[['Date','Chl']]\n",
    "    ts=pd.DataFrame(pd.date_range(start=pd.Timestamp(year[i],1,1),\n",
    "                              end=pd.Timestamp(year[i]+3,12,31),\n",
    "                              freq='D'),\n",
    "                columns=['Date'])\n",
    "    ML_Chl=ts.merge(ML_Chl,on='Date',how='left')\n",
    "\n",
    "    Comp=OB.merge(ML_Chl,on='Date',how='inner').dropna()\n",
    "    Comp.columns=['Date','OB','ML']\n",
    "    Chl_MAE.append(mean_absolute_error(Comp['OB'],Comp['ML']))\n",
    "    Chl_RMSE.append(mean_squared_error(Comp['OB'],Comp['ML'],squared=False))\n",
    "    Chl_R2.append(r2_score(Comp['OB'],Comp['ML']))\n",
    "pd.DataFrame(Chl_MAE,index=year[:14],columns=['MAE']).to_csv('MAE.csv')\n",
    "pd.DataFrame(Chl_RMSE,index=year[:14],columns=['RMSE']).to_csv('RMSE.csv')\n",
    "pd.DataFrame(Chl_R2,index=year[:14],columns=['R2']).to_csv('R2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "074a7839",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(LSTM_save_folder)\n",
    "MAE = pd.DataFrame({'NOX':NOX_MAE,'NH4':NH4_MAE,'O2':O2_MAE,'PO4':PO4_MAE,'TP':TP_MAE,'Si':Si_MAE,'Chl':Chl_MAE})\n",
    "MAE.to_csv('MAE.csv',index = False)\n",
    "\n",
    "RMSE = pd.DataFrame({'NOX':NOX_RMSE,'NH4':NH4_RMSE,'O2':O2_RMSE,'PO4':PO4_RMSE,'TP':TP_RMSE,'Si':Si_RMSE,'Chl':Chl_RMSE})\n",
    "RMSE.to_csv('RMSE.csv',index = False)\n",
    "\n",
    "R2 = pd.DataFrame({'NOX':NOX_R2,'NH4':NH4_R2,'O2':O2_R2,'PO4':PO4_R2,'TP':TP_R2,'Si':Si_R2,'Chl':Chl_R2})\n",
    "R2.to_csv('R2.csv',index = False)\n",
    "\n",
    "metrics=pd.DataFrame({'TPR':TPR,'FPR':FPR,'Kappa':Kappa},index=year[:14])\n",
    "metrics.to_csv('metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538d4a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
